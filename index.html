<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Fixed Frame Photo Capture</title>
    <style>
      body {
        background-color: #f0f0f0;
        font-family: Arial, sans-serif;
        text-align: center;
        margin: 0;
        padding: 20px;
      }
      #camera-container {
        position: relative;
        width: 720px; 
        height: 1280px;
        margin: 0 auto;
        background: #000;
      }
      #video {
        width: 100%;
        height: 100%;
        object-fit: cover;
      }
      #overlay {
        position: absolute;
        top: 0;
        left: 0;
        width: 720px;
        height: 1280px;
        pointer-events: none;
      }
      #capture-btn {
        margin-top: 20px;
        padding: 10px 20px;
        font-size: 1.2em;
      }
    </style>
  </head>
  <body>
    <h1>Fixed Frame Photo Capture</h1>
    <p>Ensure you are entirely within the fixed frame!</p>
    <div id="camera-container">
      <video id="video" autoplay playsinline></video>
      <canvas id="overlay"></canvas>
    </div>
    <button id="capture-btn">Capture Photo</button>

    <!-- Load MediaPipe FaceMesh and Camera Utils from CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>

    <script>
      // Get HTML elements
      const videoElement = document.getElementById('video');
      const overlayCanvas = document.getElementById('overlay');
      const overlayCtx = overlayCanvas.getContext('2d');
      const captureBtn = document.getElementById('capture-btn');

      // When the video metadata is loaded, update canvas dimensions to match the actual feed.
      videoElement.addEventListener('loadedmetadata', () => {
        overlayCanvas.width = videoElement.videoWidth;
        overlayCanvas.height = videoElement.videoHeight;
        // Define a frame region as fractions of width and height.
        fixedFrame.minX = 0.1 * videoElement.videoWidth;
        fixedFrame.minY = 0.1 * videoElement.videoHeight;
        fixedFrame.maxX = 0.9 * videoElement.videoWidth;
        fixedFrame.maxY = 0.9 * videoElement.videoHeight;
      });

      // Define our fixed frame region (will be updated after video metadata loads).
      let fixedFrame = {
        minX: 50,
        minY: 50,
        maxX: 670,
        maxY: 1230,
      };

      // We'll use MediaPipe FaceMesh to detect the user's nose position.
      // (For a simple validation, we use the nose landmark.)
      const faceMesh = new FaceMesh({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`;
        }
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });
      faceMesh.onResults(onResults);

      // onResults callback receives the detected landmarks.
      let currentNose = null; // will hold the nose coordinates in pixel space.
      function onResults(results) {
        // Clear overlay canvas.
        overlayCtx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

        // Draw the fixed frameâ€”color depends on validation.
        let valid = false;
        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
          // Use the first face's landmark
          const landmarks = results.multiFaceLandmarks[0];
          // In MediaPipe FaceMesh, landmark index 1 is typically near the nose tip.
          const noseLandmark = landmarks[1];
          // Convert normalized coordinates (0 to 1) into pixel space:
          currentNose = {
            x: noseLandmark.x * overlayCanvas.width,
            y: noseLandmark.y * overlayCanvas.height
          };
          valid = isFrameValid(currentNose, fixedFrame);
        }
        drawOverlay(valid);
      }

      // Draw the overlay rectangle: green if valid, red if invalid.
      function drawOverlay(isValid) {
        overlayCtx.lineWidth = 5;
        overlayCtx.strokeStyle = isValid ? 'green' : 'red';
        overlayCtx.strokeRect(fixedFrame.minX, fixedFrame.minY,
          fixedFrame.maxX - fixedFrame.minX, fixedFrame.maxY - fixedFrame.minY);
      }

      // Basic validation: check if the nose is inside the fixedFrame.
      function isFrameValid(nose, frame) {
        return (nose.x >= frame.minX && nose.x <= frame.maxX &&
                nose.y >= frame.minY && nose.y <= frame.maxY);
      }

      // Set up the camera using MediaPipe's Camera utility.
      const camera = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
        },
        width: 720,
        height: 1280
      });
      camera.start();

      // When the "Capture Photo" button is clicked, capture the current frame.
      captureBtn.addEventListener('click', () => {
        // Create an offscreen canvas to capture the video frame.
        const captureCanvas = document.createElement('canvas');
        captureCanvas.width = videoElement.videoWidth;
        captureCanvas.height = videoElement.videoHeight;
        const captureCtx = captureCanvas.getContext('2d');
        captureCtx.drawImage(videoElement, 0, 0, captureCanvas.width, captureCanvas.height);
        // Open the captured image in a new tab (for testing).
        const dataURL = captureCanvas.toDataURL('image/png');
        window.open(dataURL, '_blank');
      });
    </script>
  </body>
</html>
