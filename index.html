<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <!-- Mobile-friendly viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Responsive Fixed Frame Capture</title>
    <style>
      body {
        background-color: #f0f0f0;
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 10px;
        text-align: center;
      }
      #camera-container {
        position: relative;
        display: inline-block;
        background: #000;
        width: 100%;
        max-width: 720px;
      }
      /* Make the video responsive using object-fit */
      #video {
        width: 100%;
        height: auto;
        object-fit: contain;
        display: block;
        background: #000;
      }
      /* Overlay canvas covers the video */
      #overlay {
        position: absolute;
        top: 0;
        left: 0;
        pointer-events: none;
      }
      .btn-container {
        margin-top: 20px;
      }
      button {
        margin: 5px;
        padding: 10px 20px;
        font-size: 1.2em;
      }
    </style>
  </head>
  <body>
    <h1>Responsive Fixed Frame Capture</h1>
    <p>Please ensure you are fully within the fixed frame. (The border should be green when in frame and red if you're out.)</p>
    <div id="camera-container">
      <video id="video" autoplay playsinline></video>
      <canvas id="overlay"></canvas>
    </div>
    <div class="btn-container">
      <button id="capture-photo-btn">Capture Photo</button>
      <button id="record-video-btn">Record 10-sec Video</button>
    </div>

    <!-- Load MediaPipe FaceMesh and Camera Utils -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    
    <script>
      const videoElement = document.getElementById('video');
      const overlayCanvas = document.getElementById('overlay');
      const ctx = overlayCanvas.getContext('2d');
      const capturePhotoBtn = document.getElementById('capture-photo-btn');
      const recordVideoBtn = document.getElementById('record-video-btn');

      // Define fixed frame as fractions: 10% in from each side.
      const fixedFrameFraction = {
        minX: 0.1,
        minY: 0.1,
        maxX: 0.9,
        maxY: 0.9
      };
      let fixedFramePixels = null; // will compute after video has loaded

      // Update overlay size to match video element's displayed size
      function updateOverlaySize() {
        const rect = videoElement.getBoundingClientRect();
        overlayCanvas.width = rect.width;
        overlayCanvas.height = rect.height;
        fixedFramePixels = {
          minX: fixedFrameFraction.minX * rect.width,
          minY: fixedFrameFraction.minY * rect.height,
          maxX: fixedFrameFraction.maxX * rect.width,
          maxY: fixedFrameFraction.maxY * rect.height
        };
      }

      // Call updateOverlaySize on loadeddata and resize events.
      videoElement.addEventListener('loadedmetadata', updateOverlaySize);
      videoElement.addEventListener('loadeddata', updateOverlaySize);
      window.addEventListener('resize', updateOverlaySize);

      // FaceMesh setup to get a key landmark (nose tip) from the video feed.
      const faceMesh = new FaceMesh({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      });
      faceMesh.onResults(onFaceMeshResults);

      let currentNose = null; // stores nose position (in overlay coords)

      function onFaceMeshResults(results) {
        updateOverlaySize();
        ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

        // If face landmarks are available, use nose (index 1).
        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
          const landmarks = results.multiFaceLandmarks[0];
          const noseLandmark = landmarks[1];
          currentNose = {
            x: noseLandmark.x * overlayCanvas.width,
            y: noseLandmark.y * overlayCanvas.height
          };
        } else {
          currentNose = null;
        }
        // If no face is detected, force drawing the frame (red by default).
        let isValid = false;
        if (currentNose && fixedFramePixels) {
          isValid = (currentNose.x >= fixedFramePixels.minX && currentNose.x <= fixedFramePixels.maxX &&
                     currentNose.y >= fixedFramePixels.minY && currentNose.y <= fixedFramePixels.maxY);
        }
        // Fallback: even if face is not detected, draw the fixed frame in red.
        drawOverlay(isValid);
      }

      function drawOverlay(isValid) {
        ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
        ctx.lineWidth = 5;
        ctx.strokeStyle = isValid ? 'green' : 'red';
        if (fixedFramePixels) {
          ctx.strokeRect(
            fixedFramePixels.minX,
            fixedFramePixels.minY,
            fixedFramePixels.maxX - fixedFramePixels.minX,
            fixedFramePixels.maxY - fixedFramePixels.minY
          );
        }
      }

      // Instead of using MediaPipe's Camera util, we use standard getUserMedia with explicit constraints.
      async function initCamera() {
        try {
          const constraints = {
            audio: false,
            video: {
              facingMode: { ideal: "environment" },
              width: { ideal: 1280 },
              height: { ideal: 720 }
            }
          };
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          videoElement.srcObject = stream;
          videoElement.play();
        } catch (e) {
          alert("Error accessing camera: " + e);
          console.error(e);
        }
      }

      // Start camera and start our FaceMesh loop (using requestAnimationFrame).
      async function startFaceMeshLoop() {
        await initCamera();
        requestAnimationFrame(faceMeshLoop);
      }

      async function faceMeshLoop() {
        if (videoElement.readyState >= 2) {
          await faceMesh.send({ image: videoElement });
        }
        requestAnimationFrame(faceMeshLoop);
      }

      startFaceMeshLoop();

      // ---------------------------
      // Capture Photo (Download to user's device)
      // ---------------------------
      capturePhotoBtn.addEventListener('click', () => {
        // Use the actual video stream resolution.
        const vidW = videoElement.videoWidth;
        const vidH = videoElement.videoHeight;
        const capCanvas = document.createElement('canvas');
        capCanvas.width = vidW;
        capCanvas.height = vidH;
        const capCtx = capCanvas.getContext('2d');
        capCtx.drawImage(videoElement, 0, 0, vidW, vidH);
        capCanvas.toBlob((blob) => {
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'captured_photo.png';
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
        }, 'image/png');
      });

      // ---------------------------
      // Capture Video (10 sec, Download)
      // ---------------------------
      let mediaRecorder;
      let recordedChunks = [];
      recordVideoBtn.addEventListener('click', () => {
        if (!navigator.mediaDevices || !window.MediaRecorder) {
          alert("MediaRecorder is not supported on this device.");
          return;
        }
        recordedChunks = [];
        const stream = videoElement.srcObject;
        if (!stream) {
          alert("No camera stream available.");
          return;
        }
        try {
          mediaRecorder = new MediaRecorder(stream, { mimeType: 'video/webm' });
        } catch (e) {
          console.error("Error creating MediaRecorder:", e);
          alert("MediaRecorder error: " + e);
          return;
        }
        mediaRecorder.ondataavailable = (event) => {
          if (event.data && event.data.size > 0) {
            recordedChunks.push(event.data);
          }
        };
        mediaRecorder.onstop = () => {
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'captured_video.webm';
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
        };
        mediaRecorder.start();
        setTimeout(() => {
          if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
          }
        }, 10000);
      });
    </script>
  </body>
</html>
