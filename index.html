<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <!-- Mobile-friendly viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Responsive Fixed Frame Capture</title>
    <style>
      body {
        background-color: #f0f0f0;
        font-family: Arial, sans-serif;
        text-align: center;
        margin: 0;
        padding: 10px;
      }

      /* A flexible container that holds the video. The video will adapt to available width */
      #camera-container {
        position: relative;
        display: inline-block; /* so it can shrink or grow */
        background: #000; /* fallback background */
        width: 100%;
        max-width: 720px; /* a max width if you prefer */
      }

      /* The video is made responsive. We use object-fit: contain so it isn't cropped */
      #video {
        width: 100%;
        height: auto;
        object-fit: contain; 
        background: #000;
        display: block; /* remove extra space */
      }

      /* The overlay canvas is absolutely positioned on top of the video */
      #overlay {
        position: absolute;
        top: 0;
        left: 0;
        pointer-events: none;
        /* We'll set width/height via JS to match the displayed video size */
      }

      .btn-container {
        margin-top: 20px;
      }
      button {
        margin: 5px;
        padding: 10px 20px;
        font-size: 1.2em;
      }
    </style>
  </head>
  <body>
    <h1>Responsive Fixed Frame Capture</h1>
    <p>Stand fully inside the fixed frame. The border will turn green if in frame, red if not.</p>

    <div id="camera-container">
      <video id="video" autoplay playsinline></video>
      <canvas id="overlay"></canvas>
    </div>

    <div class="btn-container">
      <button id="capture-photo-btn">Capture Photo</button>
      <button id="record-video-btn">Record 10-sec Video</button>
    </div>
    
    <!-- MediaPipe FaceMesh and Camera Utils -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    
    <script>
      const videoElement = document.getElementById('video');
      const overlayCanvas = document.getElementById('overlay');
      const ctx = overlayCanvas.getContext('2d');
      const capturePhotoBtn = document.getElementById('capture-photo-btn');
      const recordVideoBtn = document.getElementById('record-video-btn');

      // We'll define our "fixed frame" as fractions of the displayed video size (0.1..0.9).
      // Means 10% in from each side, top, bottom.
      let fixedFrameFraction = {
        minX: 0.1,
        minY: 0.1,
        maxX: 0.9,
        maxY: 0.9
      };
      let fixedFramePixels = null; // We'll compute after we know the displayed video size.

      // A utility function: setOverlaySize() sets the canvas to match the video element's rendered size,
      // then re-computes fixedFramePixels.
      function setOverlaySize() {
        // getBoundingClientRect() tells us the displayed size in px
        const rect = videoElement.getBoundingClientRect();
        // But to actually draw, we want the "internal" size of the video feed.
        // We'll do something simpler: we assume the *DOM* size is the actual drawing size.
        overlayCanvas.width = rect.width; 
        overlayCanvas.height = rect.height;

        // In some browsers, videoWidth / videoHeight are the actual camera feed size,
        // but we want the DOM displayed size for the overlay alignment, so we use rect.

        // Now define fixedFramePixels in terms of that displayed size.
        fixedFramePixels = {
          minX: fixedFrameFraction.minX * overlayCanvas.width,
          minY: fixedFrameFraction.minY * overlayCanvas.height,
          maxX: fixedFrameFraction.maxX * overlayCanvas.width,
          maxY: fixedFrameFraction.maxY * overlayCanvas.height
        };
      }

      // We'll call setOverlaySize() whenever the window resizes or the video metadata changes.
      function updateOverlay() {
        setOverlaySize();
        // We'll also redraw the last "valid" overlay status if needed.
        drawOverlay(false); // default red on init, or you can store last state.
      }

      // The actual FaceMesh code
      const faceMesh = new FaceMesh({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      });
      faceMesh.onResults(onResults);

      let currentNose = null; // store nose position in "overlay canvas coords"

      // We'll define onResults() to do the overlay logic:
      function onResults(results) {
        // First ensure overlay matches the video's DOM size
        setOverlaySize();

        // Clear overlay
        ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
          const landmarks = results.multiFaceLandmarks[0];
          // landmark index 1 is often near the nose tip
          const nose = landmarks[1];
          // convert normalized [0..1] to overlay coords
          // we want to multiply by overlayCanvas.width, etc.
          // but the faceMesh library by default uses the video input size,
          // which may differ from the DOM size. We'll do it proportionally anyway:
          currentNose = {
            x: nose.x * overlayCanvas.width,
            y: nose.y * overlayCanvas.height
          };
        } else {
          currentNose = null;
        }

        // Validate
        let isValid = false;
        if (currentNose && fixedFramePixels) {
          if (currentNose.x >= fixedFramePixels.minX && currentNose.x <= fixedFramePixels.maxX &&
              currentNose.y >= fixedFramePixels.minY && currentNose.y <= fixedFramePixels.maxY) {
            isValid = true;
          }
        }
        drawOverlay(isValid);
      }

      // drawOverlay() draws the green or red rectangle for the fixed frame
      function drawOverlay(isValid) {
        ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
        ctx.lineWidth = 5;
        ctx.strokeStyle = isValid ? 'green' : 'red';
        if (fixedFramePixels) {
          ctx.strokeRect(
            fixedFramePixels.minX,
            fixedFramePixels.minY,
            fixedFramePixels.maxX - fixedFramePixels.minX,
            fixedFramePixels.maxY - fixedFramePixels.minY
          );
        }
      }

      // We'll set up the camera using MediaPipe's Camera utility to feed into faceMesh.
      // We'll request a 720x1280 feed, but devices may vary.
      const camera = new Camera(videoElement, {
        onFrame: async () => {
          await faceMesh.send({ image: videoElement });
        },
        width: 720,
        height: 1280
      });
      camera.start();

      // Also, whenever the window resizes, we recalc overlay.
      window.addEventListener('resize', () => {
        updateOverlay();
      });

      // When the video metadata is loaded, do an initial update.
      videoElement.addEventListener('loadedmetadata', () => {
        updateOverlay();
      });

      // ---------------------------
      // Capture Photo (with automatic download)
      // ---------------------------
      capturePhotoBtn.addEventListener('click', () => {
        // We create an offscreen canvas to draw the actual video frame
        // in its real size (the camera feed).
        const vidWidth = videoElement.videoWidth;
        const vidHeight = videoElement.videoHeight;
        const captureCanvas = document.createElement('canvas');
        captureCanvas.width = vidWidth;
        captureCanvas.height = vidHeight;
        const captureCtx = captureCanvas.getContext('2d');
        captureCtx.drawImage(videoElement, 0, 0, vidWidth, vidHeight);

        // Convert to Blob -> auto download
        captureCanvas.toBlob((blob) => {
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'captured_photo.png';
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
        }, 'image/png');
      });

      // ---------------------------
      // Video Capture (max 10s) with auto download
      // ---------------------------
      let mediaRecorder;
      let recordedChunks = [];
      recordVideoBtn.addEventListener('click', () => {
        if (!navigator.mediaDevices || !window.MediaRecorder) {
          alert("MediaRecorder is not supported on this browser.");
          return;
        }
        recordedChunks = [];
        const stream = videoElement.srcObject;
        try {
          mediaRecorder = new MediaRecorder(stream, { mimeType: 'video/webm' });
        } catch (e) {
          console.error("Error creating MediaRecorder:", e);
          alert("Error: " + e);
          return;
        }
        mediaRecorder.ondataavailable = (event) => {
          if (event.data && event.data.size > 0) {
            recordedChunks.push(event.data);
          }
        };
        mediaRecorder.onstop = () => {
          // Combine chunks -> create a Blob -> auto download
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'captured_video.webm';
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
        };
        mediaRecorder.start();

        // Stop after 10 seconds
        setTimeout(() => {
          if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
          }
        }, 10000);
      });
    </script>
  </body>
</html>
