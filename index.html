<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <!-- Mobile-friendly viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Responsive Fixed Frame Capture</title>
    <style>
      body {
        background-color: #f0f0f0;
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 10px;
        text-align: center;
      }

      #camera-container {
        position: relative;
        display: inline-block; /* let content size to its intrinsic aspect ratio */
        background: #000;
        width: 100%;
        max-width: 720px; /* optional max width */
      }

      /* The video will be scaled and keep aspect ratio with object-fit: contain. 
         This ensures no excessive zoom or crop. */
      #video {
        width: 100%;
        height: auto;
        object-fit: contain;
        display: block; /* remove small whitespace below inline-block elements */
        background: #000;
      }

      /* The overlay canvas is absolutely positioned over the video. We'll size it in JS to match the displayed size. */
      #overlay {
        position: absolute;
        top: 0;
        left: 0;
        pointer-events: none; /* clicks pass through */
      }

      .btn-container {
        margin-top: 20px;
      }
      button {
        margin: 5px;
        padding: 10px 20px;
        font-size: 1.2em;
      }
    </style>
  </head>
  <body>
    <h1>Responsive Fixed Frame Capture</h1>
    <p>Please stand within the fixed frame (green if valid, red if not).</p>

    <div id="camera-container">
      <video id="video" autoplay playsinline></video>
      <canvas id="overlay"></canvas>
    </div>

    <div class="btn-container">
      <button id="capture-photo-btn">Capture Photo</button>
      <button id="record-video-btn">Record 10-sec Video</button>
    </div>

    <!-- Load MediaPipe FaceMesh -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>

    <script>
      // HTML elements
      const videoElement = document.getElementById('video');
      const overlayCanvas = document.getElementById('overlay');
      const ctx = overlayCanvas.getContext('2d');

      const capturePhotoBtn = document.getElementById('capture-photo-btn');
      const recordVideoBtn  = document.getElementById('record-video-btn');

      // We'll define a fraction-based "fixed frame". For example, 10% in from each edge:
      const fixedFrameFraction = {
        minX: 0.1,
        minY: 0.1,
        maxX: 0.9,
        maxY: 0.9
      };
      let fixedFramePixels = null; // computed from the displayed size

      // FaceMesh instance
      const faceMesh = new FaceMesh({
        locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`
      });
      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5
      });

      let currentNose = null; // nose position in overlay coords
      faceMesh.onResults(onFaceMeshResults);

      // The plan:
      // 1) We'll getUserMedia(...) with constraints (rear camera).
      // 2) We'll display the stream in <video>.
      // 3) We'll repeatedly call faceMesh.send({image: videoElement}) in a requestAnimationFrame loop.

      async function initCamera() {
        try {
          const constraints = {
            audio: false,
            video: {
              facingMode: { ideal: 'environment' }, // request rear camera if available
              width:  { ideal: 1280 },
              height: { ideal: 720 }
            }
          };
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          videoElement.srcObject = stream;
          videoElement.play();
        } catch(e) {
          alert("Error accessing camera: " + e);
          console.error(e);
        }
      }

      // We'll sync the overlay canvas to match the displayed video size in DOM.
      function updateOverlaySize() {
        const rect = videoElement.getBoundingClientRect();
        overlayCanvas.width  = rect.width;
        overlayCanvas.height = rect.height;

        // Now define the frame in pixel coords:
        fixedFramePixels = {
          minX: rect.width  * fixedFrameFraction.minX,
          minY: rect.height * fixedFrameFraction.minY,
          maxX: rect.width  * fixedFrameFraction.maxX,
          maxY: rect.height * fixedFrameFraction.maxY
        };
      }

      function onFaceMeshResults(results) {
        // Make sure overlay is correct size
        updateOverlaySize();
        ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);

        // If face is found, get nose position
        if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
          const landmarks = results.multiFaceLandmarks[0];
          const nose = landmarks[1]; // index 1 is near nose tip
          // Convert normalized coords [0..1] to overlay coords
          currentNose = {
            x: nose.x * overlayCanvas.width,
            y: nose.y * overlayCanvas.height
          };
        } else {
          currentNose = null;
        }
        // Validate if nose is in fixedFrame
        let isValid = false;
        if (currentNose && fixedFramePixels) {
          if (currentNose.x >= fixedFramePixels.minX && currentNose.x <= fixedFramePixels.maxX &&
              currentNose.y >= fixedFramePixels.minY && currentNose.y <= fixedFramePixels.maxY) {
            isValid = true;
          }
        }
        drawOverlay(isValid);
      }

      function drawOverlay(isValid) {
        ctx.clearRect(0, 0, overlayCanvas.width, overlayCanvas.height);
        ctx.lineWidth = 5;
        ctx.strokeStyle = isValid ? 'green' : 'red';
        if (fixedFramePixels) {
          ctx.strokeRect(
            fixedFramePixels.minX,
            fixedFramePixels.minY,
            fixedFramePixels.maxX - fixedFramePixels.minX,
            fixedFramePixels.maxY - fixedFramePixels.minY
          );
        }
      }

      // We'll run FaceMesh in a requestAnimationFrame loop
      // so it continuously tracks the face from the video feed.
      async function faceMeshLoop() {
        await faceMesh.send({ image: videoElement });
        requestAnimationFrame(faceMeshLoop);
      }

      // Photo capture logic
      capturePhotoBtn.addEventListener('click', () => {
        // Capture from the real camera resolution (videoWidth / videoHeight).
        const vidW = videoElement.videoWidth;
        const vidH = videoElement.videoHeight;
        const capCanvas = document.createElement('canvas');
        capCanvas.width  = vidW;
        capCanvas.height = vidH;

        const cctx = capCanvas.getContext('2d');
        cctx.drawImage(videoElement, 0, 0, vidW, vidH);

        capCanvas.toBlob((blob) => {
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'captured_photo.png';
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
        }, 'image/png');
      });

      // Video capture logic (max 10 sec)
      let mediaRecorder;
      let recordedChunks = [];
      recordVideoBtn.addEventListener('click', () => {
        if (!navigator.mediaDevices || !window.MediaRecorder) {
          alert("MediaRecorder is not supported on this browser/device.");
          return;
        }
        recordedChunks = [];
        const stream = videoElement.srcObject;
        if (!stream) {
          alert("No camera stream available.");
          return;
        }
        try {
          mediaRecorder = new MediaRecorder(stream, { mimeType: 'video/webm' });
        } catch (e) {
          console.error("Error creating MediaRecorder:", e);
          alert("MediaRecorder error: " + e);
          return;
        }
        mediaRecorder.ondataavailable = (evt) => {
          if (evt.data && evt.data.size > 0) {
            recordedChunks.push(evt.data);
          }
        };
        mediaRecorder.onstop = () => {
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'captured_video.webm';
          document.body.appendChild(a);
          a.click();
          document.body.removeChild(a);
          URL.revokeObjectURL(url);
        };

        mediaRecorder.start();
        setTimeout(() => {
          if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
          }
        }, 10000);
      });

      // Initialize camera
      initCamera();
      // Start the FaceMesh loop once the camera is ready
      requestAnimationFrame(faceMeshLoop);

      // Whenever window resizes, recalc overlay
      window.addEventListener('resize', () => {
        updateOverlaySize();
      });

      // Also after video has loaded some data, we update.
      videoElement.addEventListener('loadeddata', () => {
        updateOverlaySize();
      });
    </script>
  </body>
</html>
